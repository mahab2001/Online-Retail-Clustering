# -*- coding: utf-8 -*-
"""Online Retail Clustering

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DueewyilrebRzlH19hhp29HAzTtkEKGE

# Customer Segmentation Using Online Retail (UCI)

**Purpose:** A step-by-step, framework for customer segmentation with clustering (K-Means + DBSCAN), designed around the Online Retail dataset but adaptable to any tabular dataset.

**What you’ll do:**
- Clean raw transactional data
- Engineer RFM-style features (Recency, Frequency, Monetary)
- Scale features and choose number of clusters (Elbow + Silhouette)
- Fit K-Means (and optionally DBSCAN)
- Visualize clusters in 2D (PCA)
- Summarize clusters and compute average spending per cluster

## 1) Import Libaries
"""

import warnings
warnings.filterwarnings("ignore")

import math
import numpy as np
import pandas as pd

from pathlib import Path

import matplotlib.pyplot as plt
# NOTE: Do not set any specific matplotlib styles or colors as per instructions.

from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

pd.set_option("display.max_columns", 100)
pd.set_option("display.width", 120)

"""
## 2) Load Data

**Expected columns (Online Retail):** `InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country`
"""

import pandas as pd

DATA_PATH = "Online Retail.xlsx"   # just the filename if uploaded
df = pd.read_excel(DATA_PATH, parse_dates=["InvoiceDate"])
df.head()
df.info()

"""
## 3) Quick EDA

**Ask:**  
- Which columns are relevant for clustering? (IDs/text may be dropped later)  
- Any missing values, duplicates, or obvious anomalies (negative quantities/prices)?
"""

print("\nMissing values per column:")
print(df.isna().sum())

print("\nDuplicate rows:", df.duplicated().sum())

print("\nBasic numeric summary:")
display(df[["Quantity","UnitPrice"]].describe())

print("\nTop countries:")
display(df["Country"].value_counts().head(10))

# Check anomalies
print("\nNegative or zero quantities sample:")
display(df[df["Quantity"] <= 0].head(10))

print("\nZero or negative prices sample:")
display(df[df["UnitPrice"] <= 0].head(10))

# Identify likely cancelled invoices (often start with 'C' in Online Retail)
if "InvoiceNo" in df.columns:
    cancelled_mask = df["InvoiceNo"].astype(str).str.startswith("C", na=False)
    print("\nCancelled invoices count:", cancelled_mask.sum())

"""
## 4) Cleaning

**Rules (sensible defaults for Online Retail):**
- Keep rows with a `CustomerID`
- Remove cancelled invoices (`InvoiceNo` starts with 'C')
- Remove negative quantities (returns) and non-positive prices
- Drop duplicates
- Create `TotalPrice = Quantity * UnitPrice`

> You can tweak these depending on your use-case (e.g., keep returns for churn analysis).
"""

def clean_online_retail(raw: pd.DataFrame,
                        drop_missing_customer=True,
                        remove_cancelled=True,
                        remove_nonpositive_qty=True,
                        remove_nonpositive_price=True):
    dfc = raw.copy()

    if drop_missing_customer and "CustomerID" in dfc.columns:
        before = len(dfc)
        dfc = dfc[dfc["CustomerID"].notna()]
        print(f"Removed missing CustomerID: {before - len(dfc)} rows")

    if remove_nonpositive_qty and "Quantity" in dfc.columns:
        before = len(dfc)
        dfc = dfc[dfc["Quantity"] > 0]
        print(f"Removed non-positive quantities: {before - len(dfc)} rows")

    if remove_nonpositive_price and "UnitPrice" in dfc.columns:
        before = len(dfc)
        dfc = dfc[dfc["UnitPrice"] > 0]
        print(f"Removed non-positive prices: {before - len(dfc)} rows")

    if remove_cancelled and "InvoiceNo" in dfc.columns:
        before = len(dfc)
        cancelled_mask = dfc["InvoiceNo"].astype(str).str.startswith("C", na=False)
        dfc = dfc[~cancelled_mask]
        print(f"Removed cancelled invoices: {before - len(dfc)} rows")

    before = len(dfc)
    dfc = dfc.drop_duplicates()
    print(f"Dropped duplicates: {before - len(dfc)} rows")

    # Create TotalPrice
    dfc["TotalPrice"] = dfc["Quantity"] * dfc["UnitPrice"]

    return dfc

df_clean = clean_online_retail(df)
print("Clean shape:", df_clean.shape)
display(df_clean.head(3))

"""## 5) Feature Engineering — RFM & Friends

We aggregate **per customer**:
- **Recency**: days since last purchase (lower = more recent)
- **Frequency**: number of invoices
- **Monetary**: sum of `TotalPrice`

Optionally add:
- **AvgBasketValue** = Monetary / Frequency
- **TotalQuantity** = sum of quantities

"""

def build_rfm(df_txn: pd.DataFrame,
              customer_col="CustomerID",
              invoice_col="InvoiceNo",
              date_col="InvoiceDate",
              price_col="TotalPrice",
              qty_col="Quantity"):
    dft = df_txn.copy()
    snapshot_date = dft[date_col].max() + pd.Timedelta(days=1)

    grp = dft.groupby(customer_col).agg(
        Recency=(date_col, lambda x: (snapshot_date - x.max()).days),
        Frequency=(invoice_col, "nunique"),
        Monetary=(price_col, "sum"),
        TotalQuantity=(qty_col, "sum"),
    )

    grp["AvgBasketValue"] = grp["Monetary"] / grp["Frequency"]
    return grp, snapshot_date

rfm, SNAPSHOT_DATE = build_rfm(df_clean)
print("RFM shape:", rfm.shape)
display(rfm.head(5))
print("Snapshot date:", SNAPSHOT_DATE)

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler

def suggest_scaler(df, threshold_skew=1):
    suggestions = {}
    for col in df.select_dtypes(include=[np.number]).columns:
        skew = df[col].skew()
        q1, q3 = df[col].quantile([0.25, 0.75])
        iqr = q3 - q1
        outliers = ((df[col] < (q1 - 1.5 * iqr)) | (df[col] > (q3 + 1.5 * iqr))).sum()

        if abs(skew) < threshold_skew and outliers < 0.01 * len(df):
            scaler = "StandardScaler"
        elif outliers > 0.01 * len(df):
            scaler = "RobustScaler"
        else:
            scaler = "MinMaxScaler"

        suggestions[col] = {
            "skewness": round(skew, 2),
            "outliers": outliers,
            "suggested_scaler": scaler
        }
    return pd.DataFrame(suggestions).T

# Example usage:
#df = pd.read_csv("Online Retail.xlsx")
scaler_suggestions = suggest_scaler(df)
print(scaler_suggestions)

"""
## 6) Feature Selection & Scaling

**Ask:**
- Which features truly represent customer behavior?
- Are they on comparable scales?
- Do we need robust scaling for outliers?

Choose your feature set below. Two presets are provided.
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, RobustScaler

def scale_features(df):
    # Copy to avoid modifying original
    df_scaled = df.copy()

    # Initialize scalers
    robust_scaler = RobustScaler()
    standard_scaler = StandardScaler()

    # ---- RobustScaler columns ----
    robust_cols = ["Quantity", "UnitPrice"]

    # Log-transform for UnitPrice to reduce extreme skewness
    df_scaled["UnitPrice"] = np.log1p(df_scaled["UnitPrice"])

    df_scaled[robust_cols] = robust_scaler.fit_transform(df_scaled[robust_cols])

    # ---- StandardScaler columns ----
    standard_cols = ["CustomerID"]
    df_scaled[standard_cols] = standard_scaler.fit_transform(df_scaled[standard_cols])

    return df_scaled

# Example usage:
features = ["Quantity", "UnitPrice", "CustomerID"]
df_scaled = scale_features(df[features])
print(df_scaled.head())

# Choose features: start simple with classic RFM
FEATURES_SIMPLE = ["Recency", "Frequency", "Monetary"]

# Or a richer set
FEATURES_RICH = ["Recency", "Frequency", "Monetary", "TotalQuantity", "AvgBasketValue"]

USE_FEATURES = FEATURES_RICH  # <<< pick one

X = rfm[USE_FEATURES].copy()

# Optionally log-transform highly skewed positives (skip Recency if zeros present)
def safe_log1p(s: pd.Series):
    # ensure strictly non-negative; if negative appears, shift
    minv = s.min()
    if minv < 0:
        s = s - minv
    return np.log1p(s)

LOG_TRANSFORM = True  # <<< toggle
if LOG_TRANSFORM:
    for col in ["Frequency", "Monetary", "TotalQuantity", "AvgBasketValue"]:
        if col in X.columns:
            X[col] = safe_log1p(X[col])

# Scale
SCALER_CHOICE = "standard"  # "standard" | "robust" | "minmax"
if SCALER_CHOICE == "standard":
    scaler = StandardScaler()
elif SCALER_CHOICE == "robust":
    scaler = RobustScaler()
else:
    scaler = MinMaxScaler()

X_scaled = scaler.fit_transform(X)
print("Scaled shape:", X_scaled.shape)

"""
## 7) Pick Number of Clusters — Elbow & Silhouette

**Ask:**
- What k is a good balance of compactness and separation?
- Is the result interpretable for the business?
"""

def plot_elbow(X_scaled, k_min=2, k_max=10):
    inertias = []
    ks = list(range(k_min, k_max+1))
    for k in ks:
        km = KMeans(n_clusters=k, n_init="auto", random_state=RANDOM_STATE)
        km.fit(X_scaled)
        inertias.append(km.inertia_)
    plt.figure()
    plt.plot(ks, inertias, marker='o')
    plt.xlabel("k")
    plt.ylabel("Inertia (Elbow)")
    plt.title("Elbow Method")
    plt.show()

def compute_silhouette(X_scaled, k_min=2, k_max=10):
    scores = []
    ks = list(range(k_min, k_max+1))
    for k in ks:
        km = KMeans(n_clusters=k, n_init="auto", random_state=RANDOM_STATE)
        labels = km.fit_predict(X_scaled)
        score = silhouette_score(X_scaled, labels)
        scores.append(score)
        print(f"k={k} silhouette={score:.4f}")
    plt.figure()
    plt.plot(ks, scores, marker='o')
    plt.xlabel("k")
    plt.ylabel("Silhouette Score")
    plt.title("Silhouette vs k")
    plt.show()

plot_elbow(X_scaled, 2, 10)
compute_silhouette(X_scaled, 2, 10)

"""
## 8) Fit Final K-Means & Attach Labels

Pick `CHOSEN_K` based on the previous step + business interpretability.
"""

CHOSEN_K = 3  # <<< set after inspecting elbow/silhouette

kmeans = KMeans(n_clusters=CHOSEN_K, n_init="auto", random_state=RANDOM_STATE)
labels = kmeans.fit_predict(X_scaled)

rfm_k = rfm.copy()
rfm_k["Cluster"] = labels

print("Cluster sizes:")
print(rfm_k["Cluster"].value_counts().sort_index())
display(rfm_k.head(5))

"""
## 9) Cluster Summary & Interpretation

**Ask:**
- How do clusters differ in behavior (means/medians)?
- Can you describe them in plain language (e.g., "High-value loyalists")?
"""

summary_mean = rfm_k.groupby("Cluster")[USE_FEATURES].mean().round(2)
summary_median = rfm_k.groupby("Cluster")[USE_FEATURES].median().round(2)
sizes = rfm_k["Cluster"].value_counts().rename("Count").sort_index()

display(pd.concat([sizes, summary_mean], axis=1))

print("\nMedians:")
display(summary_median)

"""
## 10) Visualize Clusters in 2D (PCA)
"""

pca = PCA(n_components=2, random_state=RANDOM_STATE)
X_pca = pca.fit_transform(X_scaled)

plt.figure()
plt.scatter(X_pca[:,0], X_pca[:,1], c=labels)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("K-Means Clusters (PCA projection)")
plt.show()

print("Explained variance ratio:", pca.explained_variance_ratio_)

"""
## 11) Bonus — Try DBSCAN

**Hints:**
- `eps` controls neighborhood size; `min_samples` controls density.
- Start with small grid and adjust.
- Label `-1` denotes noise (outliers).
"""

# Basic DBSCAN sweep (adjust ranges to your scale)
eps_values = [0.5, 0.8, 1.0, 1.2]
min_samples_values = [5, 10]

best = None
for eps in eps_values:
    for ms in min_samples_values:
        db = DBSCAN(eps=eps, min_samples=ms)
        db_labels = db.fit_predict(X_scaled)
        n_clusters = len(set(db_labels)) - (1 if -1 in db_labels else 0)
        n_noise = (db_labels == -1).sum()
        if n_clusters > 1:
            sil = silhouette_score(X_scaled, db_labels)
        else:
            sil = float("nan")
        print(f"eps={eps}, min_samples={ms} -> clusters={n_clusters}, noise={n_noise}, silhouette={sil}")

# Visualize one run (pick eps/min_samples after examining output)
db = DBSCAN(eps=1.0, min_samples=10)
db_labels = db.fit_predict(X_scaled)

plt.figure()
plt.scatter(X_pca[:,0], X_pca[:,1], c=db_labels)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("DBSCAN Clusters (PCA projection)")
plt.show()

"""
## 12) Average Spending per Cluster (Monetary)

**Deliverable:** Analyze average spending per cluster.
"""

avg_spend = rfm_k.groupby("Cluster")["Monetary"].mean().sort_index()
print(avg_spend)

plt.figure()
avg_spend.plot(kind="bar")
plt.title("Average Monetary Value per Cluster")
plt.xlabel("Cluster")
plt.ylabel("Average Monetary")
plt.show()